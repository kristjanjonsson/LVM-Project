{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import logging\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import scipy as sp\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except ImportError:\n",
    "    import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  rating\n",
       "0     1      1       5\n",
       "1     1      2       3\n",
       "2     1      3       4\n",
       "3     1      4       3\n",
       "4     1      5       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/ml-100k/u1.base', sep='\\t', names=['user', 'movie', 'rating', 'time'], usecols=[0,1,2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to dense matrix\n",
    "users = data.get_values()[:,0] - 1\n",
    "movies = data.get_values()[:,1] - 1\n",
    "ratings = data.get_values()[:,2]\n",
    "r = csr_matrix((ratings, (users, movies))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = r[:200, :20]\n",
    "r = r.astype(np.float32)\n",
    "\n",
    "# mark missing values\n",
    "zeros = r == 0\n",
    "r[zeros] = float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enable on-the-fly graph computations, but ignore \n",
    "# absence of intermediate test values.\n",
    "theano.config.compute_test_value = 'ignore'\n",
    "\n",
    "# Set up logging.\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class PMF(object):\n",
    "    \"\"\"Probabilistic Matrix Factorization model using pymc3.\"\"\"\n",
    "\n",
    "    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "        :param np.ndarray train: The training data to use for learning the model.\n",
    "        :param int dim: Dimensionality of the model; number of latent factors.\n",
    "        :param int alpha: Fixed precision for the likelihood function.\n",
    "        :param float std: Amount of noise to use for model initialization.\n",
    "        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n",
    "            These bounds will simply be used to cap the estimates produced for R.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.std = np.sqrt(1.0 / alpha)\n",
    "        self.bounds = bounds\n",
    "        self.data = train.copy()\n",
    "        n, m = self.data.shape\n",
    "\n",
    "        # Perform mean value imputation\n",
    "#         nan_mask = np.isnan(self.data)\n",
    "#         self.data[nan_mask] = self.data[~nan_mask].mean()\n",
    "\n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = 1 / self.data.var(axis=1).mean()\n",
    "        self.alpha_v = 1 / self.data.var(axis=0).mean()\n",
    "\n",
    "        # Specify the model.\n",
    "        logging.info('building the PMF model')\n",
    "        with pm.Model() as pmf:\n",
    "            U = pm.MvNormal(\n",
    "                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n",
    "                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "            V = pm.MvNormal(\n",
    "                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n",
    "                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "            R = pm.Normal(\n",
    "                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n",
    "                observed=pd.DataFrame(self.data))\n",
    "\n",
    "        logging.info('done building the PMF model') \n",
    "        self.model = pmf\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First define functions to save our MAP estimate after it is found.\n",
    "# We adapt these from `pymc3`'s `backends` module, where the original\n",
    "# code is used to save the traces from MCMC samples.\n",
    "def save_np_vars(vars, savedir):\n",
    "    \"\"\"Save a dictionary of numpy variables to `savedir`. We assume\n",
    "    the directory does not exist; an OSError will be raised if it does.\n",
    "    \"\"\"\n",
    "    logging.info('writing numpy vars to directory: %s' % savedir)\n",
    "    os.mkdir(savedir)\n",
    "    shapes = {}\n",
    "    for varname in vars:\n",
    "        data = vars[varname]\n",
    "        var_file = os.path.join(savedir, varname + '.txt')\n",
    "        np.savetxt(var_file, data.reshape(-1, data.size))\n",
    "        shapes[varname] = data.shape\n",
    "\n",
    "        ## Store shape information for reloading.\n",
    "        shape_file = os.path.join(savedir, 'shapes.json')\n",
    "        with open(shape_file, 'w') as sfh:\n",
    "            json.dump(shapes, sfh)\n",
    "\n",
    "\n",
    "def load_np_vars(savedir):\n",
    "    \"\"\"Load numpy variables saved with `save_np_vars`.\"\"\"\n",
    "    shape_file = os.path.join(savedir, 'shapes.json')\n",
    "    with open(shape_file, 'r') as sfh:\n",
    "        shapes = json.load(sfh)\n",
    "\n",
    "    vars = {}\n",
    "    for varname, shape in shapes.items():\n",
    "        var_file = os.path.join(savedir, varname + '.txt')\n",
    "        vars[varname] = np.loadtxt(var_file).reshape(shape)\n",
    "\n",
    "    return vars\n",
    "\n",
    "\n",
    "# Now define the MAP estimation infrastructure.\n",
    "def _map_dir(self):\n",
    "    basename = 'pmf-map-d%d' % self.dim\n",
    "    return os.path.join('data', basename)\n",
    "\n",
    "def _find_map(self):\n",
    "    \"\"\"Find mode of posterior using Powell optimization.\"\"\"\n",
    "    tstart = time.time()\n",
    "    with self.model:\n",
    "        logging.info('finding PMF MAP using Powell optimization...')\n",
    "        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell)\n",
    "\n",
    "    elapsed = int(time.time() - tstart)\n",
    "    logging.info('found PMF MAP in %d seconds' % elapsed)\n",
    "\n",
    "    # This is going to take a good deal of time to find, so let's save it.\n",
    "    save_np_vars(self._map, self.map_dir)\n",
    "\n",
    "def _load_map(self):\n",
    "    self._map = load_np_vars(self.map_dir)\n",
    "\n",
    "def _map(self):\n",
    "    try:\n",
    "        return self._map\n",
    "    except:\n",
    "        if os.path.isdir(self.map_dir):\n",
    "            self.load_map()\n",
    "        else:\n",
    "            self.find_map()\n",
    "        return self._map\n",
    "\n",
    "\n",
    "# Update our class with the new MAP infrastructure.\n",
    "PMF.find_map = _find_map\n",
    "PMF.load_map = _load_map\n",
    "PMF.map_dir = property(_map_dir)\n",
    "PMF.map = property(_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw MCMC samples.\n",
    "def _trace_dir(self):\n",
    "    basename = 'pmf-mcmc-d%d' % self.dim\n",
    "    return os.path.join('data', basename)\n",
    "\n",
    "def _draw_samples(self, nsamples=1000, njobs=2):\n",
    "    # First make sure the trace_dir does not already exist.\n",
    "    if os.path.isdir(self.trace_dir):\n",
    "        raise OSError(\n",
    "            'trace directory %s already exists. Please move or delete.' % self.trace_dir)\n",
    "    start = self.map  # use our MAP as the starting point\n",
    "    with self.model:\n",
    "        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n",
    "        step = pm.NUTS(scaling=start)\n",
    "        backend = pm.backends.Text(self.trace_dir)\n",
    "        logging.info('backing up trace to directory: %s' % self.trace_dir)\n",
    "        self.trace = pm.sample(nsamples, step, start=start, njobs=njobs, trace=backend)\n",
    "\n",
    "def _load_trace(self):\n",
    "    with self.model:\n",
    "        self.trace = pm.backends.text.load(self.trace_dir)\n",
    "\n",
    "\n",
    "# Update our class with the sampling infrastructure.\n",
    "PMF.trace_dir = property(_trace_dir)\n",
    "PMF.draw_samples = _draw_samples\n",
    "PMF.load_trace = _load_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _predict(self, U, V):\n",
    "    \"\"\"Estimate R from the given values of U and V.\"\"\"\n",
    "    R = np.dot(U, V.T)\n",
    "    n, m = R.shape\n",
    "    sample_R = np.array([\n",
    "        [np.random.normal(R[i,j], self.std) for j in range(m)]\n",
    "        for i in range(n)\n",
    "    ])\n",
    "\n",
    "    # bound ratings\n",
    "    low, high = self.bounds\n",
    "    sample_R[sample_R < low] = low\n",
    "    sample_R[sample_R > high] = high\n",
    "    return sample_R\n",
    "\n",
    "\n",
    "PMF.predict = _predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our evaluation function.\n",
    "def rmse(test_data, predicted):\n",
    "    \"\"\"Calculate root mean squared error.\n",
    "    Ignoring missing values in the test data.\n",
    "    \"\"\"\n",
    "    I = ~np.isnan(test_data)   # indicator for missing values\n",
    "    N = I.sum()                # number of non-missing values\n",
    "    sqerror = abs(test_data - predicted) ** 2  # squared error array\n",
    "    mse = sqerror[I].sum() / N                 # mean squared error\n",
    "    return np.sqrt(mse)                        # RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:building the PMF model\n",
      "INFO:root:done building the PMF model\n"
     ]
    }
   ],
   "source": [
    "# We use a fixed precision for the likelihood.\n",
    "# This reflects uncertainty in the dot product.\n",
    "# We choose 2 in the footsteps Salakhutdinov\n",
    "# Mnihof.\n",
    "ALPHA = 2\n",
    "\n",
    "# The dimensionality D; the number of latent factors.\n",
    "# We can adjust this higher to try to capture more subtle\n",
    "# characteristics of each joke. However, the higher it is,\n",
    "# the more expensive our inference procedures will be.\n",
    "# Specifically, we have D(N + M) latent variables. For our\n",
    "# Jester dataset, this means we have D(1100), so for 5\n",
    "# dimensions, we are sampling 5500 latent variables.\n",
    "DIM = 5\n",
    "\n",
    "\n",
    "pmf = PMF(r, DIM, ALPHA, std=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:finding PMF MAP using Powell optimization...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Optimization error: max, logp or dlogp at max have non-finite values. Some values may be outside of distribution support. max: {'V': array([[ 2.57977771,  2.54299568,  2.56779359,  2.51355331,  2.55846148],\n       [ 2.63088538,  2.62698609,  2.51670516,  2.51628743,  2.51168762],\n       [ 2.63217203,  2.59536215,  2.59902014,  2.63955603,  2.66147264],\n       [ 2.54581352,  2.57326731,  2.55938962,  2.66329868,  2.60306379],\n       [ 2.65120937,  2.59927454,  2.60013652,  2.59545389,  2.51541817],\n       [ 2.47097247,  2.55320745,  2.68442562,  2.58428033,  2.56930701],\n       [ 2.59365568,  2.60699381,  2.56141605,  2.60951785,  2.62964112],\n       [ 2.54988957,  2.44424054,  2.54718031,  2.72704874,  2.58974242],\n       [ 2.57430942,  2.55806105,  2.68147568,  2.60761997,  2.6266677 ],\n       [ 2.57505821,  2.55453821,  2.58058253,  2.6242412 ,  2.5571407 ],\n       [ 2.56741625,  2.58829891,  2.55232735,  2.6178619 ,  2.6069126 ],\n       [ 2.48073254,  2.57484415,  2.54057128,  2.56190249,  2.56719704],\n       [ 2.65027179,  2.59026307,  2.59794897,  2.58907659,  2.60349848],\n       [ 2.54459372,  2.57378875,  2.55872402,  2.59325928,  2.662371  ],\n       [ 2.56451242,  2.65073313,  2.63132374,  2.55439639,  2.5125286 ],\n       [ 2.59200521,  2.62780068,  2.64631992,  2.58760944,  2.52934407],\n       [ 2.55979608,  2.62811321,  2.57642305,  2.53937398,  2.51472898],\n       [ 2.58006324,  2.6264634 ,  2.5814567 ,  2.58011556,  2.64736684],\n       [ 2.55996694,  2.63597487,  2.5862997 ,  2.53800931,  2.56643423],\n       [ 2.61813973,  2.58106467,  2.62697056,  2.56214123,  2.63203009]]), 'U': array([[ 2.55635559,  2.65338568,  2.49395702,  2.50900997,  2.54034024],\n       [ 2.62344137,  2.66150397,  2.6098239 ,  2.60310136,  2.5323948 ],\n       [ 2.60398024,  2.56892235,  2.5563645 ,  2.71126712,  2.63838476],\n       [ 2.58669809,  2.64246813,  2.7653601 ,  2.54291561,  2.52206609],\n       [ 2.63399794,  2.63434188,  2.51286436,  2.58694988,  2.61358911],\n       [ 2.46671109,  2.56537958,  2.5751157 ,  2.57039381,  2.62362639],\n       [ 2.54739794,  2.5958615 ,  2.59323307,  2.57145311,  2.62669357],\n       [ 2.61765315,  2.5632136 ,  2.5072135 ,  2.57951651,  2.56644067],\n       [ 2.54979457,  2.65348862,  2.61442175,  2.64815382,  2.56763203],\n       [ 2.58752971,  2.49576938,  2.65085195,  2.62785999,  2.53169809],\n       [ 2.55009243,  2.60253638,  2.61634235,  2.60480066,  2.58653797],\n       [ 2.54199232,  2.63128909,  2.56124531,  2.57165735,  2.52686324],\n       [ 2.59752043,  2.46625153,  2.58922934,  2.4861986 ,  2.60432986],\n       [ 2.53955283,  2.53973859,  2.56598042,  2.62329402,  2.66713133],\n       [ 2.55326375,  2.64379192,  2.60149425,  2.60232494,  2.56918191],\n       [ 2.53164216,  2.54827286,  2.57391879,  2.63787433,  2.587011  ],\n       [ 2.58335364,  2.52675621,  2.60074806,  2.53391598,  2.62534478],\n       [ 2.58653226,  2.58719008,  2.53706547,  2.63985188,  2.64692168],\n       [ 2.58476312,  2.56488095,  2.61096942,  2.59784996,  2.58359731],\n       [ 2.48157553,  2.57114558,  2.51387722,  2.54451828,  2.58856312],\n       [ 2.5398164 ,  2.55045859,  2.49963067,  2.56247675,  2.55397742],\n       [ 2.5818546 ,  2.70538472,  2.63055643,  2.5304117 ,  2.54226977],\n       [ 2.62841454,  2.58994548,  2.58602239,  2.58113208,  2.57761484],\n       [ 2.5881023 ,  2.57262727,  2.64601891,  2.59771558,  2.65437948],\n       [ 2.60795585,  2.53384111,  2.63324689,  2.58542175,  2.53126902],\n       [ 2.62529402,  2.56852517,  2.59884131,  2.60413676,  2.60365878],\n       [ 2.57307183,  2.61517194,  2.61026831,  2.54570489,  2.59770218],\n       [ 2.6160415 ,  2.50416007,  2.65872725,  2.60232316,  2.5950797 ],\n       [ 2.59006262,  2.60187952,  2.52806307,  2.56606783,  2.60741188],\n       [ 2.52225536,  2.60992372,  2.56134801,  2.59100807,  2.55745376],\n       [ 2.57612234,  2.57124933,  2.62938289,  2.60593903,  2.65934614],\n       [ 2.60748554,  2.68010884,  2.64621325,  2.64790944,  2.60228932],\n       [ 2.54173413,  2.62343982,  2.59801018,  2.61573904,  2.51464627],\n       [ 2.62546608,  2.63090714,  2.55848172,  2.58311967,  2.57583559],\n       [ 2.60641492,  2.6023866 ,  2.66863378,  2.59588593,  2.68842639],\n       [ 2.6263112 ,  2.61074511,  2.56581858,  2.53033508,  2.55258434],\n       [ 2.61176492,  2.58400434,  2.61047512,  2.65004767,  2.5808849 ],\n       [ 2.64099145,  2.64610361,  2.70691135,  2.6255873 ,  2.58647599],\n       [ 2.60933722,  2.54619401,  2.56759134,  2.61761879,  2.54789649],\n       [ 2.53177793,  2.48510995,  2.62708862,  2.60575719,  2.60461471],\n       [ 2.54254685,  2.58373113,  2.57956742,  2.52338898,  2.64541003],\n       [ 2.60369287,  2.56559977,  2.50804227,  2.59099464,  2.58217584],\n       [ 2.57561709,  2.5642293 ,  2.58543363,  2.60660873,  2.61240546],\n       [ 2.6390973 ,  2.5323694 ,  2.59217005,  2.55999124,  2.53385629],\n       [ 2.57404313,  2.6039648 ,  2.60353332,  2.62311047,  2.5714477 ],\n       [ 2.60255463,  2.5700152 ,  2.48217356,  2.64493634,  2.54112133],\n       [ 2.57159713,  2.60928874,  2.60161621,  2.53848476,  2.58518534],\n       [ 2.59624159,  2.52266204,  2.50211523,  2.51437382,  2.65727098],\n       [ 2.65160069,  2.53529738,  2.62265892,  2.63064837,  2.56708683],\n       [ 2.60515332,  2.55647333,  2.69048435,  2.55727766,  2.47230976],\n       [ 2.61510442,  2.58370799,  2.55104017,  2.52128207,  2.57260814],\n       [ 2.58141834,  2.61226248,  2.61631083,  2.63610673,  2.50305469],\n       [ 2.56342844,  2.55308041,  2.61011073,  2.60524986,  2.56205695],\n       [ 2.59736234,  2.61657994,  2.61853317,  2.57945337,  2.58093834],\n       [ 2.53350815,  2.54546647,  2.62911458,  2.60474355,  2.73074557],\n       [ 2.51561429,  2.64618951,  2.53724142,  2.5492868 ,  2.56661165],\n       [ 2.48732448,  2.64543904,  2.66625909,  2.57497883,  2.53353839],\n       [ 2.62829304,  2.5703952 ,  2.46024737,  2.56206696,  2.61942931],\n       [ 2.56091102,  2.52626906,  2.59484823,  2.59282364,  2.70061071],\n       [ 2.6529186 ,  2.58842632,  2.61148216,  2.67460568,  2.56784013],\n       [ 2.61268979,  2.49816425,  2.60772545,  2.66543887,  2.6423453 ],\n       [ 2.53920618,  2.57693201,  2.62724359,  2.63899278,  2.59955939],\n       [ 2.60866157,  2.58045201,  2.60613934,  2.57173042,  2.48801879],\n       [ 2.61345049,  2.63686903,  2.63117998,  2.65501872,  2.55912176],\n       [ 2.55442977,  2.61365962,  2.56616446,  2.55403854,  2.63518376],\n       [ 2.60906887,  2.65228891,  2.51902797,  2.62225825,  2.53606511],\n       [ 2.63092091,  2.6163031 ,  2.59554433,  2.55560362,  2.64683705],\n       [ 2.58874375,  2.54277489,  2.71457237,  2.51700893,  2.66240557],\n       [ 2.53849261,  2.55961328,  2.59843295,  2.57882629,  2.53960364],\n       [ 2.68215503,  2.5975976 ,  2.62695831,  2.59512893,  2.61493722],\n       [ 2.5443412 ,  2.68168912,  2.58068537,  2.56372288,  2.54705785],\n       [ 2.57415364,  2.6953227 ,  2.59237951,  2.64644806,  2.51081276],\n       [ 2.62133287,  2.62960632,  2.59296443,  2.57108785,  2.55173869],\n       [ 2.61235595,  2.56859358,  2.5864971 ,  2.64228347,  2.58725363],\n       [ 2.58566829,  2.5205889 ,  2.67083336,  2.57758327,  2.58608096],\n       [ 2.58822884,  2.62471293,  2.60733594,  2.67054508,  2.57197291],\n       [ 2.63814149,  2.54307972,  2.69332947,  2.56939139,  2.58490516],\n       [ 2.54481382,  2.67440378,  2.54640465,  2.49758193,  2.61516563],\n       [ 2.63237959,  2.54824088,  2.54333981,  2.68359527,  2.58862697],\n       [ 2.61988419,  2.55982328,  2.59634362,  2.55932432,  2.55392811],\n       [ 2.58882458,  2.60994217,  2.57414453,  2.60845762,  2.60013097],\n       [ 2.54228441,  2.61473222,  2.5990894 ,  2.4910726 ,  2.49867672],\n       [ 2.53029554,  2.63682475,  2.62967293,  2.58072264,  2.53255955],\n       [ 2.57225144,  2.65351383,  2.52433022,  2.60751463,  2.61754527],\n       [ 2.67703996,  2.59578158,  2.62739259,  2.55490068,  2.49755558],\n       [ 2.4909265 ,  2.52480287,  2.57317734,  2.58377388,  2.60122628],\n       [ 2.63078279,  2.51946805,  2.50155006,  2.6165782 ,  2.61881157],\n       [ 2.54040238,  2.52904156,  2.67711129,  2.57662241,  2.54118394],\n       [ 2.5810068 ,  2.61548995,  2.56671073,  2.55047246,  2.57083188],\n       [ 2.63114171,  2.58979249,  2.53196162,  2.67771038,  2.61591114],\n       [ 2.56988703,  2.56495478,  2.5361976 ,  2.50611989,  2.5617231 ],\n       [ 2.52330972,  2.55397979,  2.62142609,  2.52518922,  2.53846415],\n       [ 2.60541859,  2.59671563,  2.61555824,  2.51977939,  2.54716275],\n       [ 2.62633651,  2.56109328,  2.43090792,  2.63184736,  2.56351927],\n       [ 2.44670676,  2.62204417,  2.55870641,  2.52827401,  2.54287013],\n       [ 2.61888184,  2.68694728,  2.56579257,  2.58458294,  2.53305474],\n       [ 2.58497131,  2.62471119,  2.5391824 ,  2.62570464,  2.6113301 ],\n       [ 2.66377649,  2.62731254,  2.6023663 ,  2.60520981,  2.605935  ],\n       [ 2.50279134,  2.60297889,  2.52635001,  2.5705698 ,  2.55496417],\n       [ 2.56528256,  2.54842674,  2.58880644,  2.50043534,  2.64801664],\n       [ 2.61155051,  2.54595887,  2.64955872,  2.54759948,  2.58144331],\n       [ 2.55828232,  2.6112165 ,  2.58136479,  2.51522217,  2.58590474],\n       [ 2.54225602,  2.56698996,  2.55865629,  2.57084756,  2.57071523],\n       [ 2.55102585,  2.58741859,  2.67901522,  2.62981873,  2.58622185],\n       [ 2.61717312,  2.52799451,  2.5872486 ,  2.61156963,  2.62368637],\n       [ 2.55028975,  2.65033423,  2.60832993,  2.6281481 ,  2.58502788],\n       [ 2.457975  ,  2.5845308 ,  2.58237824,  2.50924903,  2.53601406],\n       [ 2.55781661,  2.59715744,  2.6357882 ,  2.57878859,  2.55061913],\n       [ 2.58916755,  2.53037173,  2.6167237 ,  2.57514512,  2.57478496],\n       [ 2.68796836,  2.51200116,  2.59455491,  2.70652561,  2.59570062],\n       [ 2.58929745,  2.57010829,  2.46548366,  2.63678168,  2.57906541],\n       [ 2.58081453,  2.60243626,  2.55780087,  2.61834468,  2.63101784],\n       [ 2.65524497,  2.55555663,  2.56277771,  2.54802015,  2.63279013],\n       [ 2.52623031,  2.60089876,  2.56258872,  2.63634312,  2.56098606],\n       [ 2.56006901,  2.61698424,  2.56748695,  2.59120269,  2.65107717],\n       [ 2.58934889,  2.60442782,  2.52312738,  2.56357715,  2.62994533],\n       [ 2.56153863,  2.59673438,  2.56014551,  2.56997107,  2.62085321],\n       [ 2.4496593 ,  2.56641094,  2.59503435,  2.52433971,  2.6021628 ],\n       [ 2.51518306,  2.61286499,  2.62793436,  2.59114142,  2.61065667],\n       [ 2.67354451,  2.55999806,  2.60788845,  2.597069  ,  2.63481743],\n       [ 2.54123025,  2.53118098,  2.59221091,  2.58644643,  2.66334848],\n       [ 2.6377275 ,  2.56202215,  2.56650375,  2.51889593,  2.55089588],\n       [ 2.55269685,  2.60509837,  2.57415873,  2.55865473,  2.62242562],\n       [ 2.62888977,  2.56867538,  2.55329922,  2.56751776,  2.5812543 ],\n       [ 2.50039332,  2.61321231,  2.64385495,  2.5862803 ,  2.56040777],\n       [ 2.60635146,  2.52203603,  2.58358895,  2.55126346,  2.55049349],\n       [ 2.56433767,  2.62803881,  2.50009208,  2.57789974,  2.67714154],\n       [ 2.60126301,  2.63598598,  2.54732195,  2.60260081,  2.68953755],\n       [ 2.58308463,  2.64419164,  2.53067107,  2.53814562,  2.5920903 ],\n       [ 2.62046197,  2.60065913,  2.61896577,  2.58667164,  2.55165429],\n       [ 2.61489033,  2.64161511,  2.71948948,  2.57885432,  2.58212361],\n       [ 2.5437278 ,  2.56681247,  2.58174887,  2.56673601,  2.59242431],\n       [ 2.67552137,  2.62287346,  2.65484989,  2.52292029,  2.52618122],\n       [ 2.50055076,  2.52401603,  2.60139721,  2.58877652,  2.48209265],\n       [ 2.60383551,  2.57469065,  2.66471958,  2.58806904,  2.58139908],\n       [ 2.60651733,  2.54429735,  2.64008788,  2.56581032,  2.56906805],\n       [ 2.60028992,  2.59600804,  2.64121053,  2.63086452,  2.56271808],\n       [ 2.56285455,  2.56929716,  2.63433648,  2.65471032,  2.6415869 ],\n       [ 2.57659124,  2.62463424,  2.56095864,  2.62597785,  2.61055314],\n       [ 2.64045083,  2.49308213,  2.52246248,  2.59507082,  2.54552329],\n       [ 2.62233326,  2.59310977,  2.52360049,  2.61788375,  2.74356899],\n       [ 2.6063242 ,  2.58731787,  2.60039769,  2.57306567,  2.56073019],\n       [ 2.47552984,  2.54073219,  2.57309362,  2.68125206,  2.61707288],\n       [ 2.53173257,  2.53111847,  2.5153777 ,  2.60512473,  2.6785063 ],\n       [ 2.52692562,  2.59675121,  2.51503054,  2.53822236,  2.58196553],\n       [ 2.60967305,  2.62932768,  2.59177521,  2.58125356,  2.67935983],\n       [ 2.55450278,  2.59634586,  2.60297466,  2.64720586,  2.64627189],\n       [ 2.64681972,  2.55817485,  2.5910126 ,  2.65438892,  2.64355228],\n       [ 2.50256876,  2.60462994,  2.56647919,  2.56707868,  2.53785861],\n       [ 2.67568871,  2.61171328,  2.53067784,  2.6018943 ,  2.53806559],\n       [ 2.57023442,  2.62750515,  2.60829343,  2.59503256,  2.58782397],\n       [ 2.52470866,  2.562007  ,  2.55364531,  2.50418613,  2.67435999],\n       [ 2.5968804 ,  2.55781914,  2.56573742,  2.55682601,  2.49215307],\n       [ 2.61563014,  2.66142801,  2.61306801,  2.62260931,  2.64245422],\n       [ 2.52469443,  2.59306396,  2.65357852,  2.62494019,  2.55083592],\n       [ 2.58635905,  2.60013483,  2.58129765,  2.51581299,  2.58099786],\n       [ 2.483671  ,  2.6662339 ,  2.57861088,  2.62402004,  2.63747993],\n       [ 2.53852697,  2.61242054,  2.60279608,  2.57255884,  2.56613077],\n       [ 2.50461382,  2.57775798,  2.56881368,  2.62012242,  2.60198233],\n       [ 2.56502204,  2.63146124,  2.55493548,  2.63609793,  2.60406327],\n       [ 2.59336784,  2.56134286,  2.59748391,  2.64762938,  2.58538748],\n       [ 2.5848363 ,  2.58482844,  2.57750335,  2.58164048,  2.52066402],\n       [ 2.6502644 ,  2.57152336,  2.59554701,  2.59675543,  2.61237061],\n       [ 2.55345904,  2.53615489,  2.64072925,  2.60515719,  2.55603594],\n       [ 2.59866792,  2.5701538 ,  2.59677673,  2.57544975,  2.51678489],\n       [ 2.5369702 ,  2.62929358,  2.61035212,  2.57324188,  2.63269657],\n       [ 2.61366692,  2.52816545,  2.62654178,  2.59926641,  2.58495431],\n       [ 2.73581677,  2.56039458,  2.58629659,  2.61717478,  2.62416581],\n       [ 2.55147492,  2.66996746,  2.60156443,  2.59644553,  2.6481311 ],\n       [ 2.61220307,  2.6494251 ,  2.61194081,  2.5587641 ,  2.60755932],\n       [ 2.66953781,  2.58794198,  2.56651136,  2.60433469,  2.65391916],\n       [ 2.52937462,  2.686624  ,  2.58594794,  2.6257601 ,  2.64636645],\n       [ 2.57239427,  2.62023494,  2.58742323,  2.57802673,  2.62612905],\n       [ 2.65029837,  2.59717527,  2.63770648,  2.65206158,  2.70028859],\n       [ 2.61027836,  2.59777355,  2.51278655,  2.60455611,  2.517971  ],\n       [ 2.62154201,  2.67323751,  2.53598665,  2.64546544,  2.57389459],\n       [ 2.56212521,  2.54065671,  2.69469126,  2.61235867,  2.57619867],\n       [ 2.51991999,  2.54189829,  2.61815586,  2.59967197,  2.62005968],\n       [ 2.5958829 ,  2.61358985,  2.62967192,  2.63876979,  2.59201396],\n       [ 2.62518557,  2.5590017 ,  2.62933282,  2.5197974 ,  2.6194374 ],\n       [ 2.6182219 ,  2.67199684,  2.54362935,  2.59124213,  2.55934615],\n       [ 2.59368487,  2.54813678,  2.64875336,  2.54701438,  2.59520805],\n       [ 2.69058816,  2.62395882,  2.57615359,  2.61328164,  2.66392683],\n       [ 2.56448821,  2.61875161,  2.53053458,  2.53992139,  2.60347291],\n       [ 2.59959704,  2.69178874,  2.49526391,  2.5267305 ,  2.51074665],\n       [ 2.54809477,  2.56684897,  2.52338045,  2.56962444,  2.62750346],\n       [ 2.51567867,  2.57310332,  2.56080745,  2.65466072,  2.67677777],\n       [ 2.55984587,  2.5815974 ,  2.5850523 ,  2.60552695,  2.56284715],\n       [ 2.56744851,  2.64816944,  2.63254193,  2.49635281,  2.56627941],\n       [ 2.57111326,  2.57626432,  2.69079496,  2.60591272,  2.71187472],\n       [ 2.64184051,  2.60151088,  2.68113001,  2.61021213,  2.55363807],\n       [ 2.55094052,  2.49408376,  2.58117572,  2.51164653,  2.67410388],\n       [ 2.50089648,  2.55478412,  2.60945507,  2.60633111,  2.58363866],\n       [ 2.5654633 ,  2.51045318,  2.71282012,  2.61487002,  2.68242993],\n       [ 2.61558973,  2.62708592,  2.5563765 ,  2.53728233,  2.45802846],\n       [ 2.63002151,  2.50203786,  2.65499168,  2.60365064,  2.56759317],\n       [ 2.54337858,  2.59079696,  2.66402619,  2.58137267,  2.56887559],\n       [ 2.5512057 ,  2.58763663,  2.60018712,  2.5072966 ,  2.60897376],\n       [ 2.6100793 ,  2.60102071,  2.5736796 ,  2.57310195,  2.54874528],\n       [ 2.59931431,  2.69073286,  2.58585364,  2.56512703,  2.66128967]]), 'R_missing': array([ 6.47117769,  6.47117769,  6.47117769, ...,  6.47117769,\n        6.47117769,  6.47117769])} logp: array(nan) dlogp: array([         nan,          nan,          nan, ...,  55.28266475,\n        54.60763776,  55.29783894])Check that 1) you don't have hierarchical parameters, these will lead to points with infinite density. 2) your distribution logp's are properly specified. Specific issues: \nV.logp bad: nan\nV.dlogp bad at idx: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),) with values: [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\nU.logp bad: nan\nU.dlogp bad at idx: (array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n       832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844,\n       845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857,\n       858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870,\n       871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883,\n       884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n       897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n       910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922,\n       923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935,\n       936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948,\n       949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961,\n       962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974,\n       975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987,\n       988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),) with values: [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-25a0594f1092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4b0817081e4c>\u001b[0m in \u001b[0;36m_find_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finding PMF MAP using Powell optimization...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_MAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin_powell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/robert/Documents/ucsd/271/venv/lib/python3.5/site-packages/pymc3/tuning/starting.py\u001b[0m in \u001b[0;36mfind_MAP\u001b[0;34m(start, vars, fmin, return_raw, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m                          \u001b[0;34m\"density. 2) your distribution logp's are \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                          \u001b[0;34m\"properly specified. Specific issues: \\n\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                          specific_errors)\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mmx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Optimization error: max, logp or dlogp at max have non-finite values. Some values may be outside of distribution support. max: {'V': array([[ 2.57977771,  2.54299568,  2.56779359,  2.51355331,  2.55846148],\n       [ 2.63088538,  2.62698609,  2.51670516,  2.51628743,  2.51168762],\n       [ 2.63217203,  2.59536215,  2.59902014,  2.63955603,  2.66147264],\n       [ 2.54581352,  2.57326731,  2.55938962,  2.66329868,  2.60306379],\n       [ 2.65120937,  2.59927454,  2.60013652,  2.59545389,  2.51541817],\n       [ 2.47097247,  2.55320745,  2.68442562,  2.58428033,  2.56930701],\n       [ 2.59365568,  2.60699381,  2.56141605,  2.60951785,  2.62964112],\n       [ 2.54988957,  2.44424054,  2.54718031,  2.72704874,  2.58974242],\n       [ 2.57430942,  2.55806105,  2.68147568,  2.60761997,  2.6266677 ],\n       [ 2.57505821,  2.55453821,  2.58058253,  2.6242412 ,  2.5571407 ],\n       [ 2.56741625,  2.58829891,  2.55232735,  2.6178619 ,  2.6069126 ],\n       [ 2.48073254,  2.57484415,  2.54057128,  2.56190249,  2.56719704],\n       [ 2.65027179,  2.59026307,  2.59794897,  2.58907659,  2.60349848],\n       [ 2.54459372,  2.57378875,  2.55872402,  2.59325928,  2.662371  ],\n       [ 2.56451242,  2.65073313,  2.63132374,  2.55439639,  2.5125286 ],\n       [ 2.59200521,  2.62780068,  2.64631992,  2.58760944,  2.52934407],\n       [ 2.55979608,  2.62811321,  2.57642305,  2.53937398,  2.51472898],\n       [ 2.58006324,  2.6264634 ,  2.5814567 ,  2.58011556,  2.64736684],\n       [ 2.55996694,  2.63597487,  2.5862997 ,  2.53800931,  2.56643423],\n       [ 2.61813973,  2.58106467,  2.62697056,  2.56214123,  2.63203009]]), 'U': array([[ 2.55635559,  2.65338568,  2.49395702,  2.50900997,  2.54034024],\n       [ 2.62344137,  2.66150397,  2.6098239 ,  2.60310136,  2.5323948 ],\n       [ 2.60398024,  2.56892235,  2.5563645 ,  2.71126712,  2.63838476],\n       [ 2.58669809,  2.64246813,  2.7653601 ,  2.54291561,  2.52206609],\n       [ 2.63399794,  2.63434188,  2.51286436,  2.58694988,  2.61358911],\n       [ 2.46671109,  2.56537958,  2.5751157 ,  2.57039381,  2.62362639],\n       [ 2.54739794,  2.5958615 ,  2.59323307,  2.57145311,  2.62669357],\n       [ 2.61765315,  2.5632136 ,  2.5072135 ,  2.57951651,  2.56644067],\n       [ 2.54979457,  2.65348862,  2.61442175,  2.64815382,  2.56763203],\n       [ 2.58752971,  2.49576938,  2.65085195,  2.62785999,  2.53169809],\n       [ 2.55009243,  2.60253638,  2.61634235,  2.60480066,  2.58653797],\n       [ 2.54199232,  2.63128909,  2.56124531,  2.57165735,  2.52686324],\n       [ 2.59752043,  2.46625153,  2.58922934,  2.4861986 ,  2.60432986],\n       [ 2.53955283,  2.53973859,  2.56598042,  2.62329402,  2.66713133],\n       [ 2.55326375,  2.64379192,  2.60149425,  2.60232494,  2.56918191],\n       [ 2.53164216,  2.54827286,  2.57391879,  2.63787433,  2.587011  ],\n       [ 2.58335364,  2.52675621,  2.60074806,  2.53391598,  2.62534478],\n       [ 2.58653226,  2.58719008,  2.53706547,  2.63985188,  2.64692168],\n       [ 2.58476312,  2.56488095,  2.61096942,  2.59784996,  2.58359731],\n       [ 2.48157553,  2.57114558,  2.51387722,  2.54451828,  2.58856312],\n       [ 2.5398164 ,  2.55045859,  2.49963067,  2.56247675,  2.55397742],\n       [ 2.5818546 ,  2.70538472,  2.63055643,  2.5304117 ,  2.54226977],\n       [ 2.62841454,  2.58994548,  2.58602239,  2.58113208,  2.57761484],\n       [ 2.5881023 ,  2.57262727,  2.64601891,  2.59771558,  2.65437948],\n       [ 2.60795585,  2.53384111,  2.63324689,  2.58542175,  2.53126902],\n       [ 2.62529402,  2.56852517,  2.59884131,  2.60413676,  2.60365878],\n       [ 2.57307183,  2.61517194,  2.61026831,  2.54570489,  2.59770218],\n       [ 2.6160415 ,  2.50416007,  2.65872725,  2.60232316,  2.5950797 ],\n       [ 2.59006262,  2.60187952,  2.52806307,  2.56606783,  2.60741188],\n       [ 2.52225536,  2.60992372,  2.56134801,  2.59100807,  2.55745376],\n       [ 2.57612234,  2.57124933,  2.62938289,  2.60593903,  2.65934614],\n       [ 2.60748554,  2.68010884,  2.64621325,  2.64790944,  2.60228932],\n       [ 2.54173413,  2.62343982,  2.59801018,  2.61573904,  2.51464627],\n       [ 2.62546608,  2.63090714,  2.55848172,  2.58311967,  2.57583559],\n       [ 2.60641492,  2.6023866 ,  2.66863378,  2.59588593,  2.68842639],\n       [ 2.6263112 ,  2.61074511,  2.56581858,  2.53033508,  2.55258434],\n       [ 2.61176492,  2.58400434,  2.61047512,  2.65004767,  2.5808849 ],\n       [ 2.64099145,  2.64610361,  2.70691135,  2.6255873 ,  2.58647599],\n       [ 2.60933722,  2.54619401,  2.56759134,  2.61761879,  2.54789649],\n       [ 2.53177793,  2.48510995,  2.62708862,  2.60575719,  2.60461471],\n       [ 2.54254685,  2.58373113,  2.57956742,  2.52338898,  2.64541003],\n       [ 2.60369287,  2.56559977,  2.50804227,  2.59099464,  2.58217584],\n       [ 2.57561709,  2.5642293 ,  2.58543363,  2.60660873,  2.61240546],\n       [ 2.6390973 ,  2.5323694 ,  2.59217005,  2.55999124,  2.53385629],\n       [ 2.57404313,  2.6039648 ,  2.60353332,  2.62311047,  2.5714477 ],\n       [ 2.60255463,  2.5700152 ,  2.48217356,  2.64493634,  2.54112133],\n       [ 2.57159713,  2.60928874,  2.60161621,  2.53848476,  2.58518534],\n       [ 2.59624159,  2.52266204,  2.50211523,  2.51437382,  2.65727098],\n       [ 2.65160069,  2.53529738,  2.62265892,  2.63064837,  2.56708683],\n       [ 2.60515332,  2.55647333,  2.69048435,  2.55727766,  2.47230976],\n       [ 2.61510442,  2.58370799,  2.55104017,  2.52128207,  2.57260814],\n       [ 2.58141834,  2.61226248,  2.61631083,  2.63610673,  2.50305469],\n       [ 2.56342844,  2.55308041,  2.61011073,  2.60524986,  2.56205695],\n       [ 2.59736234,  2.61657994,  2.61853317,  2.57945337,  2.58093834],\n       [ 2.53350815,  2.54546647,  2.62911458,  2.60474355,  2.73074557],\n       [ 2.51561429,  2.64618951,  2.53724142,  2.5492868 ,  2.56661165],\n       [ 2.48732448,  2.64543904,  2.66625909,  2.57497883,  2.53353839],\n       [ 2.62829304,  2.5703952 ,  2.46024737,  2.56206696,  2.61942931],\n       [ 2.56091102,  2.52626906,  2.59484823,  2.59282364,  2.70061071],\n       [ 2.6529186 ,  2.58842632,  2.61148216,  2.67460568,  2.56784013],\n       [ 2.61268979,  2.49816425,  2.60772545,  2.66543887,  2.6423453 ],\n       [ 2.53920618,  2.57693201,  2.62724359,  2.63899278,  2.59955939],\n       [ 2.60866157,  2.58045201,  2.60613934,  2.57173042,  2.48801879],\n       [ 2.61345049,  2.63686903,  2.63117998,  2.65501872,  2.55912176],\n       [ 2.55442977,  2.61365962,  2.56616446,  2.55403854,  2.63518376],\n       [ 2.60906887,  2.65228891,  2.51902797,  2.62225825,  2.53606511],\n       [ 2.63092091,  2.6163031 ,  2.59554433,  2.55560362,  2.64683705],\n       [ 2.58874375,  2.54277489,  2.71457237,  2.51700893,  2.66240557],\n       [ 2.53849261,  2.55961328,  2.59843295,  2.57882629,  2.53960364],\n       [ 2.68215503,  2.5975976 ,  2.62695831,  2.59512893,  2.61493722],\n       [ 2.5443412 ,  2.68168912,  2.58068537,  2.56372288,  2.54705785],\n       [ 2.57415364,  2.6953227 ,  2.59237951,  2.64644806,  2.51081276],\n       [ 2.62133287,  2.62960632,  2.59296443,  2.57108785,  2.55173869],\n       [ 2.61235595,  2.56859358,  2.5864971 ,  2.64228347,  2.58725363],\n       [ 2.58566829,  2.5205889 ,  2.67083336,  2.57758327,  2.58608096],\n       [ 2.58822884,  2.62471293,  2.60733594,  2.67054508,  2.57197291],\n       [ 2.63814149,  2.54307972,  2.69332947,  2.56939139,  2.58490516],\n       [ 2.54481382,  2.67440378,  2.54640465,  2.49758193,  2.61516563],\n       [ 2.63237959,  2.54824088,  2.54333981,  2.68359527,  2.58862697],\n       [ 2.61988419,  2.55982328,  2.59634362,  2.55932432,  2.55392811],\n       [ 2.58882458,  2.60994217,  2.57414453,  2.60845762,  2.60013097],\n       [ 2.54228441,  2.61473222,  2.5990894 ,  2.4910726 ,  2.49867672],\n       [ 2.53029554,  2.63682475,  2.62967293,  2.58072264,  2.53255955],\n       [ 2.57225144,  2.65351383,  2.52433022,  2.60751463,  2.61754527],\n       [ 2.67703996,  2.59578158,  2.62739259,  2.55490068,  2.49755558],\n       [ 2.4909265 ,  2.52480287,  2.57317734,  2.58377388,  2.60122628],\n       [ 2.63078279,  2.51946805,  2.50155006,  2.6165782 ,  2.61881157],\n       [ 2.54040238,  2.52904156,  2.67711129,  2.57662241,  2.54118394],\n       [ 2.5810068 ,  2.61548995,  2.56671073,  2.55047246,  2.57083188],\n       [ 2.63114171,  2.58979249,  2.53196162,  2.67771038,  2.61591114],\n       [ 2.56988703,  2.56495478,  2.5361976 ,  2.50611989,  2.5617231 ],\n       [ 2.52330972,  2.55397979,  2.62142609,  2.52518922,  2.53846415],\n       [ 2.60541859,  2.59671563,  2.61555824,  2.51977939,  2.54716275],\n       [ 2.62633651,  2.56109328,  2.43090792,  2.63184736,  2.56351927],\n       [ 2.44670676,  2.62204417,  2.55870641,  2.52827401,  2.54287013],\n       [ 2.61888184,  2.68694728,  2.56579257,  2.58458294,  2.53305474],\n       [ 2.58497131,  2.62471119,  2.5391824 ,  2.62570464,  2.6113301 ],\n       [ 2.66377649,  2.62731254,  2.6023663 ,  2.60520981,  2.605935  ],\n       [ 2.50279134,  2.60297889,  2.52635001,  2.5705698 ,  2.55496417],\n       [ 2.56528256,  2.54842674,  2.58880644,  2.50043534,  2.64801664],\n       [ 2.61155051,  2.54595887,  2.64955872,  2.54759948,  2.58144331],\n       [ 2.55828232,  2.6112165 ,  2.58136479,  2.51522217,  2.58590474],\n       [ 2.54225602,  2.56698996,  2.55865629,  2.57084756,  2.57071523],\n       [ 2.55102585,  2.58741859,  2.67901522,  2.62981873,  2.58622185],\n       [ 2.61717312,  2.52799451,  2.5872486 ,  2.61156963,  2.62368637],\n       [ 2.55028975,  2.65033423,  2.60832993,  2.6281481 ,  2.58502788],\n       [ 2.457975  ,  2.5845308 ,  2.58237824,  2.50924903,  2.53601406],\n       [ 2.55781661,  2.59715744,  2.6357882 ,  2.57878859,  2.55061913],\n       [ 2.58916755,  2.53037173,  2.6167237 ,  2.57514512,  2.57478496],\n       [ 2.68796836,  2.51200116,  2.59455491,  2.70652561,  2.59570062],\n       [ 2.58929745,  2.57010829,  2.46548366,  2.63678168,  2.57906541],\n       [ 2.58081453,  2.60243626,  2.55780087,  2.61834468,  2.63101784],\n       [ 2.65524497,  2.55555663,  2.56277771,  2.54802015,  2.63279013],\n       [ 2.52623031,  2.60089876,  2.56258872,  2.63634312,  2.56098606],\n       [ 2.56006901,  2.61698424,  2.56748695,  2.59120269,  2.65107717],\n       [ 2.58934889,  2.60442782,  2.52312738,  2.56357715,  2.62994533],\n       [ 2.56153863,  2.59673438,  2.56014551,  2.56997107,  2.62085321],\n       [ 2.4496593 ,  2.56641094,  2.59503435,  2.52433971,  2.6021628 ],\n       [ 2.51518306,  2.61286499,  2.62793436,  2.59114142,  2.61065667],\n       [ 2.67354451,  2.55999806,  2.60788845,  2.597069  ,  2.63481743],\n       [ 2.54123025,  2.53118098,  2.59221091,  2.58644643,  2.66334848],\n       [ 2.6377275 ,  2.56202215,  2.56650375,  2.51889593,  2.55089588],\n       [ 2.55269685,  2.60509837,  2.57415873,  2.55865473,  2.62242562],\n       [ 2.62888977,  2.56867538,  2.55329922,  2.56751776,  2.5812543 ],\n       [ 2.50039332,  2.61321231,  2.64385495,  2.5862803 ,  2.56040777],\n       [ 2.60635146,  2.52203603,  2.58358895,  2.55126346,  2.55049349],\n       [ 2.56433767,  2.62803881,  2.50009208,  2.57789974,  2.67714154],\n       [ 2.60126301,  2.63598598,  2.54732195,  2.60260081,  2.68953755],\n       [ 2.58308463,  2.64419164,  2.53067107,  2.53814562,  2.5920903 ],\n       [ 2.62046197,  2.60065913,  2.61896577,  2.58667164,  2.55165429],\n       [ 2.61489033,  2.64161511,  2.71948948,  2.57885432,  2.58212361],\n       [ 2.5437278 ,  2.56681247,  2.58174887,  2.56673601,  2.59242431],\n       [ 2.67552137,  2.62287346,  2.65484989,  2.52292029,  2.52618122],\n       [ 2.50055076,  2.52401603,  2.60139721,  2.58877652,  2.48209265],\n       [ 2.60383551,  2.57469065,  2.66471958,  2.58806904,  2.58139908],\n       [ 2.60651733,  2.54429735,  2.64008788,  2.56581032,  2.56906805],\n       [ 2.60028992,  2.59600804,  2.64121053,  2.63086452,  2.56271808],\n       [ 2.56285455,  2.56929716,  2.63433648,  2.65471032,  2.6415869 ],\n       [ 2.57659124,  2.62463424,  2.56095864,  2.62597785,  2.61055314],\n       [ 2.64045083,  2.49308213,  2.52246248,  2.59507082,  2.54552329],\n       [ 2.62233326,  2.59310977,  2.52360049,  2.61788375,  2.74356899],\n       [ 2.6063242 ,  2.58731787,  2.60039769,  2.57306567,  2.56073019],\n       [ 2.47552984,  2.54073219,  2.57309362,  2.68125206,  2.61707288],\n       [ 2.53173257,  2.53111847,  2.5153777 ,  2.60512473,  2.6785063 ],\n       [ 2.52692562,  2.59675121,  2.51503054,  2.53822236,  2.58196553],\n       [ 2.60967305,  2.62932768,  2.59177521,  2.58125356,  2.67935983],\n       [ 2.55450278,  2.59634586,  2.60297466,  2.64720586,  2.64627189],\n       [ 2.64681972,  2.55817485,  2.5910126 ,  2.65438892,  2.64355228],\n       [ 2.50256876,  2.60462994,  2.56647919,  2.56707868,  2.53785861],\n       [ 2.67568871,  2.61171328,  2.53067784,  2.6018943 ,  2.53806559],\n       [ 2.57023442,  2.62750515,  2.60829343,  2.59503256,  2.58782397],\n       [ 2.52470866,  2.562007  ,  2.55364531,  2.50418613,  2.67435999],\n       [ 2.5968804 ,  2.55781914,  2.56573742,  2.55682601,  2.49215307],\n       [ 2.61563014,  2.66142801,  2.61306801,  2.62260931,  2.64245422],\n       [ 2.52469443,  2.59306396,  2.65357852,  2.62494019,  2.55083592],\n       [ 2.58635905,  2.60013483,  2.58129765,  2.51581299,  2.58099786],\n       [ 2.483671  ,  2.6662339 ,  2.57861088,  2.62402004,  2.63747993],\n       [ 2.53852697,  2.61242054,  2.60279608,  2.57255884,  2.56613077],\n       [ 2.50461382,  2.57775798,  2.56881368,  2.62012242,  2.60198233],\n       [ 2.56502204,  2.63146124,  2.55493548,  2.63609793,  2.60406327],\n       [ 2.59336784,  2.56134286,  2.59748391,  2.64762938,  2.58538748],\n       [ 2.5848363 ,  2.58482844,  2.57750335,  2.58164048,  2.52066402],\n       [ 2.6502644 ,  2.57152336,  2.59554701,  2.59675543,  2.61237061],\n       [ 2.55345904,  2.53615489,  2.64072925,  2.60515719,  2.55603594],\n       [ 2.59866792,  2.5701538 ,  2.59677673,  2.57544975,  2.51678489],\n       [ 2.5369702 ,  2.62929358,  2.61035212,  2.57324188,  2.63269657],\n       [ 2.61366692,  2.52816545,  2.62654178,  2.59926641,  2.58495431],\n       [ 2.73581677,  2.56039458,  2.58629659,  2.61717478,  2.62416581],\n       [ 2.55147492,  2.66996746,  2.60156443,  2.59644553,  2.6481311 ],\n       [ 2.61220307,  2.6494251 ,  2.61194081,  2.5587641 ,  2.60755932],\n       [ 2.66953781,  2.58794198,  2.56651136,  2.60433469,  2.65391916],\n       [ 2.52937462,  2.686624  ,  2.58594794,  2.6257601 ,  2.64636645],\n       [ 2.57239427,  2.62023494,  2.58742323,  2.57802673,  2.62612905],\n       [ 2.65029837,  2.59717527,  2.63770648,  2.65206158,  2.70028859],\n       [ 2.61027836,  2.59777355,  2.51278655,  2.60455611,  2.517971  ],\n       [ 2.62154201,  2.67323751,  2.53598665,  2.64546544,  2.57389459],\n       [ 2.56212521,  2.54065671,  2.69469126,  2.61235867,  2.57619867],\n       [ 2.51991999,  2.54189829,  2.61815586,  2.59967197,  2.62005968],\n       [ 2.5958829 ,  2.61358985,  2.62967192,  2.63876979,  2.59201396],\n       [ 2.62518557,  2.5590017 ,  2.62933282,  2.5197974 ,  2.6194374 ],\n       [ 2.6182219 ,  2.67199684,  2.54362935,  2.59124213,  2.55934615],\n       [ 2.59368487,  2.54813678,  2.64875336,  2.54701438,  2.59520805],\n       [ 2.69058816,  2.62395882,  2.57615359,  2.61328164,  2.66392683],\n       [ 2.56448821,  2.61875161,  2.53053458,  2.53992139,  2.60347291],\n       [ 2.59959704,  2.69178874,  2.49526391,  2.5267305 ,  2.51074665],\n       [ 2.54809477,  2.56684897,  2.52338045,  2.56962444,  2.62750346],\n       [ 2.51567867,  2.57310332,  2.56080745,  2.65466072,  2.67677777],\n       [ 2.55984587,  2.5815974 ,  2.5850523 ,  2.60552695,  2.56284715],\n       [ 2.56744851,  2.64816944,  2.63254193,  2.49635281,  2.56627941],\n       [ 2.57111326,  2.57626432,  2.69079496,  2.60591272,  2.71187472],\n       [ 2.64184051,  2.60151088,  2.68113001,  2.61021213,  2.55363807],\n       [ 2.55094052,  2.49408376,  2.58117572,  2.51164653,  2.67410388],\n       [ 2.50089648,  2.55478412,  2.60945507,  2.60633111,  2.58363866],\n       [ 2.5654633 ,  2.51045318,  2.71282012,  2.61487002,  2.68242993],\n       [ 2.61558973,  2.62708592,  2.5563765 ,  2.53728233,  2.45802846],\n       [ 2.63002151,  2.50203786,  2.65499168,  2.60365064,  2.56759317],\n       [ 2.54337858,  2.59079696,  2.66402619,  2.58137267,  2.56887559],\n       [ 2.5512057 ,  2.58763663,  2.60018712,  2.5072966 ,  2.60897376],\n       [ 2.6100793 ,  2.60102071,  2.5736796 ,  2.57310195,  2.54874528],\n       [ 2.59931431,  2.69073286,  2.58585364,  2.56512703,  2.66128967]]), 'R_missing': array([ 6.47117769,  6.47117769,  6.47117769, ...,  6.47117769,\n        6.47117769,  6.47117769])} logp: array(nan) dlogp: array([         nan,          nan,          nan, ...,  55.28266475,\n        54.60763776,  55.29783894])Check that 1) you don't have hierarchical parameters, these will lead to points with infinite density. 2) your distribution logp's are properly specified. Specific issues: \nV.logp bad: nan\nV.dlogp bad at idx: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),) with values: [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\nU.logp bad: nan\nU.dlogp bad at idx: (array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n       832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844,\n       845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857,\n       858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870,\n       871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883,\n       884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n       897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n       910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922,\n       923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935,\n       936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948,\n       949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961,\n       962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974,\n       975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987,\n       988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),) with values: [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]"
     ]
    }
   ],
   "source": [
    "pmf.find_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_map(pmf_model, train, test):\n",
    "    U = pmf_model.map['U']\n",
    "    V = pmf_model.map['V']\n",
    "\n",
    "    # Make predictions and calculate RMSE on train & test sets.\n",
    "    predictions = pmf_model.predict(U, V)\n",
    "    train_rmse = rmse(train, predictions)\n",
    "    test_rmse = rmse(test, predictions)\n",
    "    overfit = test_rmse - train_rmse\n",
    "\n",
    "    # Print report.\n",
    "    print('PMF MAP training RMSE: %.5f' % train_rmse)\n",
    "    print('PMF MAP testing RMSE:  %.5f' % test_rmse)\n",
    "    print('Train/test difference: %.5f' % overfit)\n",
    "\n",
    "    return test_rmse\n",
    "\n",
    "\n",
    "# Add eval function to PMF class.\n",
    "PMF.eval_map = eval_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate PMF MAP estimates.\n",
    "pmf_map_rmse = pmf.eval_map(r, r)\n",
    "pmf_improvement = baselines['mom'] - pmf_map_rmse\n",
    "print('PMF MAP Improvement:   %.5f' % pmf_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
